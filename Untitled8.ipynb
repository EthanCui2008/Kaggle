{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHPeEi8dl3vOcpnu1QAgCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EthanCui2008/Kaggle/blob/Main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t3_DqG6JOnQn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "http://incompleteideas.net/MountainCar/MountainCar1.cp\n",
        "permalink: https://perma.cc/6Z2N-PFWC\n",
        "\"\"\"\n",
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.envs.classic_control import utils\n",
        "from gym.error import DependencyNotInstalled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "    ### Description\n",
        "\n",
        "    The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically\n",
        "    at the bottom of a sinusoidal valley, with the only possible actions being the accelerations\n",
        "    that can be applied to the car in either direction. The goal of the MDP is to strategically\n",
        "    accelerate the car to reach the goal state on top of the right hill. There are two versions\n",
        "    of the mountain car domain in gym: one with discrete actions and one with continuous.\n",
        "    This version is the one with discrete actions.\n",
        "\n",
        "    This MDP first appeared in [Andrew Moore's PhD Thesis (1990)](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf)\n",
        "\n",
        "    ```\n",
        "    @TECHREPORT{Moore90efficientmemory-based,\n",
        "        author = {Andrew William Moore},\n",
        "        title = {Efficient Memory-based Learning for Robot Control},\n",
        "        institution = {University of Cambridge},\n",
        "        year = {1990}\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### Observation Space\n",
        "\n",
        "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
        "\n",
        "    | Num | Observation                          | Min  | Max | Unit         |\n",
        "    |-----|--------------------------------------|------|-----|--------------|\n",
        "    | 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
        "    | 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
        "\n",
        "    ### Action Space\n",
        "\n",
        "    There are 3 discrete deterministic actions:\n",
        "\n",
        "    | Num | Observation             | Value | Unit         |\n",
        "    |-----|-------------------------|-------|--------------|\n",
        "    | 0   | Accelerate to the left  | Inf   | position (m) |\n",
        "    | 1   | Don't accelerate        | Inf   | position (m) |\n",
        "    | 2   | Accelerate to the right | Inf   | position (m) |\n",
        "\n",
        "    ### Transition Dynamics:\n",
        "\n",
        "    Given an action, the mountain car follows the following transition dynamics:\n",
        "\n",
        "    *velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
        "\n",
        "    *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>*\n",
        "\n",
        "    where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic with the velocity set to 0\n",
        "    upon collision with the wall. The position is clipped to the range `[-1.2, 0.6]` and\n",
        "    velocity is clipped to the range `[-0.07, 0.07]`.\n",
        "\n",
        "\n",
        "    ### Reward:\n",
        "\n",
        "    The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is\n",
        "    penalised with a reward of -1 for each timestep.\n",
        "\n",
        "    ### Starting State\n",
        "\n",
        "    The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*.\n",
        "    The starting velocity of the car is always assigned to 0.\n",
        "\n",
        "    ### Episode End\n",
        "\n",
        "    The episode ends if either of the following happens:\n",
        "    1. Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
        "    2. Truncation: The length of the episode is 200.\n",
        "\n",
        "\n",
        "    ### Arguments\n",
        "\n",
        "    ```\n",
        "    gym.make('MountainCar-v0')\n",
        "    ```\n",
        "\n",
        "    ### Version History\n",
        "\n",
        "    * v0: Initial versions release (1.0.0)\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "VP-57voMO3z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MountainCarEnv(gym.Env):\n",
        "    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):\n",
        "        self.min_position = -1.2\n",
        "        self.max_position = 0.6\n",
        "        self.max_speed = 0.07\n",
        "        self.goal_position = 0.5\n",
        "        self.goal_velocity = goal_velocity\n",
        "\n",
        "        self.force = 0.001\n",
        "        self.gravity = 0.0025\n",
        "\n",
        "        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n",
        "        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.screen_width = 600\n",
        "        self.screen_height = 400\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
        "\n",
        "    def step(self, action: int):\n",
        "        assert self.action_space.contains(\n",
        "            action\n",
        "        ), f\"{action!r} ({type(action)}) invalid\"\n",
        "\n",
        "        position, velocity = self.state\n",
        "        velocity += (action - 1) * self.force + math.cos(3 * position) * (-self.gravity)\n",
        "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
        "        position += velocity\n",
        "        position = np.clip(position, self.min_position, self.max_position)\n",
        "        if position == self.min_position and velocity < 0:\n",
        "            velocity = 0\n",
        "\n",
        "        terminated = bool(\n",
        "            position >= self.goal_position and velocity >= self.goal_velocity\n",
        "        )\n",
        "        reward = -1.0\n",
        "\n",
        "        self.state = (position, velocity)\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
        "        # state/observations.\n",
        "        low, high = utils.maybe_parse_reset_bounds(options, -0.6, -0.4)\n",
        "        self.state = np.array([self.np_random.uniform(low=low, high=high), 0])\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return np.array(self.state, dtype=np.float32), {}\n",
        "\n",
        "    def _height(self, xs):\n",
        "        return np.sin(3 * xs) * 0.45 + 0.55\n",
        "\n",
        "    def get_keys_to_action(self):\n",
        "        # Control with left and right arrow keys.\n",
        "        return {(): 1, (276,): 0, (275,): 2, (275, 276): 1}\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False"
      ],
      "metadata": {
        "id": "DQ8tK4iLOw9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}